{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venvs/pnma/lib/python3.10/site-packages/graspologic/models/edge_swaps.py:215: NumbaDeprecationWarning: \u001b[1mThe keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  _edge_swap_numba = nb.jit(_edge_swap, nopython=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from graspologic.embed import ClassicalMDS\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from scipy.interpolate import RBFInterpolator, LinearNDInterpolator\n",
    "import random\n",
    "\n",
    "import perturbations\n",
    "from string import ascii_letters\n",
    "ascii_letters += \" \"\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "          (k_proj): QuantLinear()\n",
       "          (o_proj): QuantLinear()\n",
       "          (q_proj): QuantLinear()\n",
       "          (v_proj): QuantLinear()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (act_fn): SiLUActivation()\n",
       "          (down_proj): QuantLinear()\n",
       "          (gate_proj): QuantLinear()\n",
       "          (up_proj): QuantLinear()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\",\n",
    "                                            output_hidden_states=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venvs/pnma/lib/python3.10/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s>[INST] <<SYS>>\n",
      "Answer only with 'yes' or 'no'. Other answers will be discarded.\n",
      "<</SYS>>\n",
      "\n",
      "Was RA Fisher a great man? [/INST]\n",
      "Yes</s>\n"
     ]
    }
   ],
   "source": [
    "instructions = \"Answer only with 'yes' or 'no'. Other answers will be discarded.\"\n",
    "prompt = \"Was RA Fisher a great man?\"\n",
    "\n",
    "prompt_template = f'''<s>[INST] <<SYS>>\n",
    "{instructions}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]\n",
    "'''\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.9, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=3)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_data_file_path = '/home/ubuntu/pnma/files/RA-Fisher.csv'\n",
    "fisher_data = pd.read_csv(fisher_data_file_path)\n",
    "strings = fisher_data['string']\n",
    "labels = fisher_data['label']\n",
    "C = np.array([0 if ell=='statistics' else 1 for ell in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 1734.76it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "perturbations_dict = {}\n",
    "length_list = [0, 1, 10, 100, 1000]\n",
    "n_perturbations=10\n",
    "\n",
    "p_strings = []\n",
    "for i in range(n_perturbations):\n",
    "    s = \"\"\n",
    "    for j in range(1000):\n",
    "        s+= random.choice(ascii_letters)\n",
    "\n",
    "    p_strings.append(s)\n",
    "\n",
    "\n",
    "for i, p in enumerate(tqdm(length_list)):    \n",
    "    perturbations_dict[p] = {}\n",
    "    perturber = AppendPerturbation(p)\n",
    "    for s in strings:\n",
    "        if p == 0:\n",
    "            perturbations_dict[p][s] = [perturber.perturb(s, new_appendix=False)]\n",
    "        else:\n",
    "            perturbations_dict[p][s] = []\n",
    "            for ii in range(n_perturbations):\n",
    "                perturber.appendix=p_strings[ii][:p]\n",
    "                perturbations_dict[p][s].append(perturber.perturb(s, new_appendix=False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(model, input_ids, tokenizer, generate_kwargs={'max_new_tokens': 3}):    \n",
    "    output = model.generate(inputs=input_ids, **generate_kwargs)\n",
    "    response = tokenizer.decode(output[0])\n",
    "\n",
    "    response = response.split('[/INST]')[-1]\n",
    "    response = response.split('</s>')[0]\n",
    "    response = response.lower()\n",
    "\n",
    "    response = response.split(' ')[-1]\n",
    "    response = response.split('.')[0]\n",
    "\n",
    "    return response\n",
    "    \n",
    "\n",
    "def get_embedding(model, input_ids):\n",
    "    output =  model(input_ids)\n",
    "    embedding = np.mean(output.hidden_states[0].detach().cpu().numpy(), axis=1).flatten()\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def get_formatted_prompt(prompt, context=\"\", instruction=None):\n",
    "    if instruction is None:\n",
    "        instruction = \"Answer only with 'yes' or 'no' in English.\"\n",
    "\n",
    "    if len(context) == 0:\n",
    "        context_and_prompt = prompt\n",
    "\n",
    "    else:\n",
    "        context_and_prompt = f'{context} {prompt}'\n",
    "\n",
    "    formatted_prompt = f'''<s>[INST] <<SYS>>\n",
    "        {instruction}\n",
    "        <</SYS>>\n",
    "        \n",
    "        {context_and_prompt} [/INST]'''\n",
    "\n",
    "    return formatted_prompt\n",
    "    \n",
    "\n",
    "def get_strings_stratified(strings, labels, n=10, S=10):\n",
    "    if S > n:\n",
    "        S = n\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    \n",
    "    stats_strings_indices = np.random.choice(np.where(labels==unique_labels[1])[0], S, replace=True) \n",
    "    eugenics_strings_indices = np.random.choice(np.where(labels==unique_labels[0])[0], n-S, replace=True)\n",
    "\n",
    "    selected_strings = [strings[i] for i in stats_strings_indices] + [strings[i] for i in eugenics_strings_indices]\n",
    "    random.shuffle(selected_strings)\n",
    "        \n",
    "    return selected_strings\n",
    "\n",
    "\n",
    "def combine_strings(string_list):\n",
    "    s = \"\"\n",
    "    for s_ in string_list:\n",
    "        s += \" \" + s_\n",
    "\n",
    "    return s[1:]\n",
    "\n",
    "\n",
    "def get_context_and_prompt(prompt, strings, labels, C=2, S=2):\n",
    "    string_list = get_strings_stratified(strings, labels, C, S)\n",
    "    context_string = combine_strings(string_list)\n",
    "\n",
    "    return context_string + \" \" + prompt\n",
    "\n",
    "def estimate_p(model, input_ids, tokenizer, generate_kwargs, n_responses, max_c):\n",
    "    c=0\n",
    "    valid_responses=0\n",
    "    phat=0\n",
    "    while valid_responses<n_responses and c<max_c:\n",
    "        response = get_response(model, input_ids, tokenizer, generate_kwargs)\n",
    "\n",
    "        # print(response)\n",
    "\n",
    "        if response.encode('utf-8') in YES_LIST:\n",
    "            valid_responses+=1\n",
    "            phat+=1\n",
    "        elif response.encode('utf-8') in NO_LIST:\n",
    "            valid_responses+=1\n",
    "        c+=1\n",
    "\n",
    "    phat /= valid_responses\n",
    "\n",
    "    return phat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 50/50 [03:53<00:00,  4.68s/it]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "NO_LIST = [b'no', b'\\xe2\\x9d\\x8c', b'\\xe2\\x98\\xb9', b'\\xf0\\x9f\\x98\\x90']\n",
    "YES_LIST = [b'yes', b'\\xf0\\x9f\\x98\\x8a']\n",
    "\n",
    "skip_words = ['not', 'was']\n",
    "n_responses = 20\n",
    "\n",
    "generate_kwargs = {\n",
    "    'temperature':0.8, \n",
    "    'do_sample':True, \n",
    "    'top_p':0.95, \n",
    "    'top_k':40, \n",
    "    'max_new_tokens':10\n",
    "}\n",
    "\n",
    "\n",
    "# embeddings_dict = {}\n",
    "# phats_dict = {}\n",
    "\n",
    "max_c=100\n",
    "\n",
    "for length in [0]:\n",
    "    if length not in embeddings_dict.keys():\n",
    "        embeddings_dict[length] = {}\n",
    "        phats_dict[length] = {}\n",
    "    for s in tqdm(strings):\n",
    "        if s not in embeddings_dict[length].keys():\n",
    "            embeddings_dict[length][s] = []\n",
    "            phats_dict[length][s] = []\n",
    "    \n",
    "        if length == 0:\n",
    "            formatted_prompt = get_formatted_prompt(prompt, context=s)\n",
    "            \n",
    "            input_ids = tokenizer(formatted_prompt, return_tensors='pt').input_ids.cuda()\n",
    "            \n",
    "            embeddings_dict[length][s].append(get_embedding(model, input_ids))\n",
    "\n",
    "            phats_dict[length][s].append(estimate_p(model, input_ids, tokenizer, generate_kwargs, n_responses, max_c))\n",
    "\n",
    "            del input_ids\n",
    "\n",
    "            continue\n",
    "\n",
    "        n_perturbed_strings_completed=len(phats_dict[length][s])\n",
    "\n",
    "        for i, perturbed_string in enumerate(perturbations_dict[length][s][n_perturbed_strings_completed:], n_perturbed_strings_completed):\n",
    "            formatted_prompt = get_formatted_prompt(prompt, context=perturbed_string)\n",
    "    \n",
    "            input_ids = tokenizer(formatted_prompt, return_tensors='pt').input_ids.cuda()\n",
    "        \n",
    "            embedding = get_embedding(model, input_ids)\n",
    "            embeddings_dict[length][s].append(embedding)\n",
    "        \n",
    "            phat = estimate_p(model, input_ids, tokenizer, generate_kwargs, n_responses, max_c)\n",
    "            phats_dict[length][s].append(phat)\n",
    "\n",
    "            del input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "embeddings_and_phats = {'embeddings': embeddings_dict, 'phats': phats_dict}\n",
    "pickle.dump(embeddings_and_phats, open('/home/ubuntu/data/embeddings_and_phats_with_paired_appendix.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnma",
   "language": "python",
   "name": "pnma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
